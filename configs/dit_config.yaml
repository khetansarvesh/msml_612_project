# DiT (Diffusion Transformer) Configuration for MNIST

# Random seed for full reproducibility (can be overridden by SEED environment variable)
seed: 42

# ============================================================================
# Data configuration
# ============================================================================
data:
  dataset: "mnist" # "mnist" or "cifar10"
  batch_size: 128 # MNIST is simpler, can use larger batch
  shuffle: True
  num_workers: 0 # number of data loading workers (0 = main process)

# ============================================================================
# Model configuration - DiT specific
# ============================================================================
model:
  model_type: "dit" # "dit" or "unet"
  # Image properties (MNIST: 28Ã—28 grayscale)
  image_size: 28
  image_channels: 1 # Grayscale
  # DIT (Diffusion Transformer) architecture
  depth: 8 # Smaller than CIFAR since MNIST is simpler
  num_heads: 6
  hidden_dim: 768 # Smaller hidden dim for faster training

# ============================================================================
# Training configuration
# ============================================================================
training:
  num_epochs: 50 # Production training
  learning_rate: 1.0e-4
  optimizer: "adam"
  adam_betas: [0.9, 0.999]
  adam_eps: 1.0e-8
  weight_decay: 0.0
  # Checkpoint settings
  checkpoint_path: "models/dit_models/"
  save_every_n_epochs: 10
  # Resume from checkpoint (set to None or empty string to skip)
  resume_from: null
  # Spot instance options (for AWS training)
  save_every_epoch: false # Enable to save after every epoch (useful for spot instances, but slower)
  auto_resume: true # Automatically resume from latest checkpoint if available

# ============================================================================
# Diffusion process configuration
# ============================================================================
diffusion:
  # Number of diffusion timesteps
  num_steps: 1000
  # Beta schedule parameters
  beta_start: 1.0e-4
  beta_end: 0.02
  beta_schedule: "linear" # "linear" or "cosine"

# ============================================================================
# Inference/Sampling configuration
# ============================================================================
inference:
  num_samples: 16
  num_grid_rows: 4
  class_label: 3 # digit to generate (0-9 for MNIST)

# ============================================================================
# Device and debugging
# ============================================================================
device: "cuda" # "cuda" or "cpu"
deterministic_algorithms: False # enforce deterministic PyTorch algorithms
